# -*- coding: utf-8 -*-
"""Week_0_Tic_tac_toe_Vacuum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vnEs_nWoIyqbfvw3fOvyq52aMAtV7Etq
"""

import math

# Initialize board
board =  [' ', 'O', 'O',
         ' ', 'X', 'O',
         'X', ' ', ' ']
human_player = 'X'
ai_player = 'O'

# Winning combinations
WIN_COMBINATIONS = [
    (0, 1, 2), (3, 4, 5), (6, 7, 8),  # Rows
    (0, 3, 6), (1, 4, 7), (2, 5, 8),  # Columns
    (0, 4, 8), (2, 4, 6)             # Diagonals
]

def print_board(board):
    """Prints the Tic-Tac-Toe board."""
    print(f"| {board[0]} | {board[1]} | {board[2]} |")
    print("-----------")
    print(f"| {board[3]} | {board[4]} | {board[5]} |")
    print("-----------")
    print(f"| {board[6]} | {board[7]} | {board[8]} |")

def check_win(board, player):
    """Checks if the given player has won."""
    for combo in WIN_COMBINATIONS:
        if board[combo[0]] == board[combo[1]] == board[combo[2]] == player:
            return True
    return False

def is_draw(board):
    """Checks if the game is a draw."""
    return ' ' not in board and not check_win(board, human_player) and not check_win(board, ai_player)

def get_empty_cells(board):
    """Returns a list of empty cell indices."""
    return [i for i, cell in enumerate(board) if cell == ' ']

def evaluate_board(board):
    """
    Evaluates the board from the AI's perspective.
    +1 for AI win, -1 for human win, 0 for draw or ongoing.
    """
    if check_win(board, ai_player):
        return 1
    elif check_win(board, human_player):
        return -1
    elif is_draw(board):
        return 0
    return 0

def minimax(board, depth, maximizing_player):
    """
    Minimax algorithm to find the optimal move.
    `depth` limits the search depth for performance in larger trees.
    `maximizing_player` is True for AI (O), False for human (X).
    """

    score = evaluate_board(board)
    if score != 0:
        return score

    if is_draw(board):
        return 0

    if maximizing_player:
        max_eval = -math.inf
        for move in get_empty_cells(board):
            board[move] = ai_player
            eval = minimax(board, depth + 1, False)
            board[move] = ' '  # Undo the move
            max_eval = max(max_eval, eval)
        return max_eval
    else:
        min_eval = math.inf
        for move in get_empty_cells(board):
            board[move] = human_player
            eval = minimax(board, depth + 1, True)
            board[move] = ' '  # Undo the move
            min_eval = min(min_eval, eval)
        return min_eval

def find_best_move(board):
    """Finds the optimal move for the AI using minimax."""
    best_eval = -math.inf
    best_move = -1

    for move in get_empty_cells(board):
        board[move] = ai_player
        eval = minimax(board, 0, False)
        board[move] = ' '  # Undo the move

        if eval > best_eval:
            best_eval = eval
            best_move = move
    return best_move

def player_move(board, player_icon):
    """Gets user input for a move."""
    while True:
        try:
            move = int(input(f"Player {player_icon}, enter your move (1-9): ")) - 1
            if 0 <= move < 9 and board[move] == ' ':
                board[move] = player_icon
                break
            else:
                print("Invalid move. Try again.")
        except ValueError:
            print("Invalid input. Enter a number between 1 and 9.")

def visualize_state_space_tree(board, current_player, depth=0, prefix=""):
    """Recursively generates and displays the state space tree with move evaluations."""
    indent = "  " * depth

    # Evaluate the current board state for the AI
    eval_score = evaluate_board(board)
    outcome_str = ""
    if eval_score == 1:
        outcome_str = "(AI Wins)"
    elif eval_score == -1:
        outcome_str = "(Human Wins)"
    elif is_draw(board):
        outcome_str = "(Draw)"

    print(f"{indent}{prefix}Board State: {board} {outcome_str}")

    if check_win(board, human_player) or check_win(board, ai_player) or is_draw(board):
        return  # Terminal state

    for move in get_empty_cells(board):
        temp_board = list(board)  # Create a copy to avoid modifying the original
        temp_board[move] = current_player

        # Determine the outcome of this specific move (for the current player)
        move_outcome = ""
        if check_win(temp_board, current_player):
            move_outcome = f"(Player {current_player} Wins)"
        elif is_draw(temp_board):
            move_outcome = "(Draw)"

        visualize_state_space_tree(temp_board, 'X' if current_player == 'O' else 'O', depth + 1, f"-> Move {move + 1} {move_outcome} : ")


# Main game loop
print("Welcome to Tic-Tac-Toe!")
print("Here's the initial board:")
print_board(board)

current_turn_player = human_player

while True:
    if current_turn_player == human_player:
        player_move(board, human_player)
    else:
        print("\nAI is making its move...")
        ai_move = find_best_move(board)
        board[ai_move] = ai_player
        print(f"AI chose position {ai_move + 1}")

    print("\nCurrent board:")
    print_board(board)

    # Visualize and evaluate the state space tree after each move
    print("\n--- State Space Tree for Remaining Possibilities ---")
    visualize_state_space_tree(list(board), current_turn_player) # Pass a copy of the board

    if check_win(board, human_player):
        print(f"\nPlayer {human_player} wins! Congratulations!")
        break
    elif check_win(board, ai_player):
        print(f"\nPlayer {ai_player} wins! Better luck next time!")
        break
    elif is_draw(board):
        print("\nIt's a draw!")
        break

    current_turn_player = ai_player if current_turn_player == human_player else human_player

env = {
    "A": input("Is Room A dirty? (yes/no): ").strip().lower().replace("yes","Dirty").replace("no","Clean"),
    "B": input("Is Room B dirty? (yes/no): ").strip().lower().replace("yes","Dirty").replace("no","Clean")
}
location = input("Where is the agent? (A/B): ").strip().upper()

# SIMPLE REFLEX AGENT
def simple_reflex(env, loc):
    if env[loc] == "Dirty":
        print(f"{loc} is dirty → CLEAN")
        env[loc] = "Clean"
    else:
        loc = "A" if loc=="B" else "B"
        print(f"{loc} is clean → Move to {loc}")
    return loc

#  GOAL-BASED AGENT
def goal_based(env, loc):
    if env[loc] == "Dirty":
        print(f"{loc} is dirty → CLEAN")
        env[loc] = "Clean"
    elif "Dirty" in env.values():
        loc = "A" if env["A"]=="Dirty" else "B"
        print(f"Move to {loc}")
    else:
        print("Goal Reached: All rooms Clean ")
    return loc

print("\nInitial:", env, "Agent at", location)

print("\n--- Simple Reflex Agent ---")
for _ in range(4):
    location = simple_reflex(env, location)
    print("State:", env)

print("\n--- Goal-Based Agent ---")
for _ in range(4):
    location = goal_based(env, location)
    print("State:", env)